{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling\n",
    "\n",
    "In the previous example we saw how to extract information from a single webpage, but what if we wanted to do this for a whole website? This is called web crawling, it's a form of recursion, extract all info and links from a page, follow those links to new pages, extract all links...repeat. When crawling we should be considerate of the servers hosting the pages we crawl, if we request pages to fast we can overload the servers, and they may even block our IP address. We could crawl with just the request package however after more than a few pages this start to become more difficult. Since the web is messy not all pages will be written in the same way or may contain malformated html, we will need to use alot exception handling to ensure that our programs don't crash. In addition what if we want to make concurrent request, use rotating proxies or excute javascript? For all of these reasons sometimes it simpler to use a more robust scraping framework which has many useful featues built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "Scrapy is a complete framework for writting web crawlers, a programme to extract structured data from a website. It provides a series of command line tools and a shell to make the process easier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The Project\n",
    "\n",
    "\n",
    "We'll make a simple spider that starts on a wikipedia page extracts the title of the page and then follows the links to all the other pages. To start a new project open a terminal and run:\n",
    "\n",
    "```\n",
    "scrapy startproject wikiSpider\n",
    "\n",
    "```\n",
    "\n",
    "This will will create a bunch of files that look like this.\n",
    "\n",
    "```\n",
    "\n",
    "wikiSpider\n",
    "├── scrapy.cfg\n",
    "├── wiki.log\n",
    "└── wikiSpider\n",
    "    ├── __init__.py\n",
    "    ├── items.py\n",
    "    ├── middlewares.py\n",
    "    ├── pipelines.py\n",
    "    ├── __pycache__\n",
    "    │   ├── __init__.cpython-36.pyc\n",
    "    │   ├── items.cpython-36.pyc\n",
    "    │   └── settings.cpython-36.pyc\n",
    "    ├── settings.py\n",
    "    └── spiders\n",
    "        ├── articleSpider.py\n",
    "        ├── __init__.py\n",
    "        └── __pycache__\n",
    "            ├── Article.cpython-36.pyc\n",
    "            ├── articleSpider.cpython-36.pyc\n",
    "            └── __init__.cpython-36.pyc\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items\n",
    "\n",
    "Before we start scrapping we should have an idea of what information we want from the webpage. Threrefore we can start by defining an `Item` class (taht should inherient from the `Item` class),as this is what well use to store our collected data. We can think of an `Item` like a python dict, just a container to store information, however they give us extra protection if we make a mistake. For example if we make a typo with one of the keys in a normal dict nothing will happen, with `Items` however if we try to add or access a field thats not in the item it will give an error. In addition `Item` are used in the `pipelines.py` file which we can use to further process the data we extract, such as outputing it to a database. In the items.py add the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scrapy import Item, Field\n",
    "\n",
    "class Article(Item):\n",
    "    # define the fields for your item here like\n",
    "    title = Field()\n",
    "    content = Field()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shell\n",
    "\n",
    "The shell is the perfect place to test our code before we copy them into our spider script, it allows us to interactively build the spider, hopefully leading to less buggy code. To start it in a terminal run the bellow from within the project folder.\n",
    "```\n",
    "scrapy shell\n",
    "```\n",
    "\n",
    "We can then download the html using:\n",
    "\n",
    "```python\n",
    "fetch('https://en.wikipedia.org/wiki/London')\n",
    "```\n",
    "\n",
    "We now have a `response` object that we can use css selectors, xpath or regex to get data from. Use chrome devtools to inspect the page and figure out the selector for the element of intrest. After we can use the `.css` method to apply the selector to the response object.\n",
    "\n",
    "```python\n",
    "div = response.css('#mw-content-text > div > div:nth-child(134) > div > div.thumbcaption')\n",
    "```\n",
    "\n",
    "The `.css` method will return a selector list, so afterward we can apply another selector.\n",
    "\n",
    "```python\n",
    "l =  div.css('div.legend::text')\n",
    "```\n",
    "\n",
    "Notice the psudeo selector `::text` which we're using to extract text from each div. Finally we can apply a regex to get the clean text we want. Unlike `.css` the regex returns us a list of strings, not a selector list. \n",
    "\n",
    "\n",
    "```python\n",
    "l.re('\\w.+\\)')\n",
    "```\n",
    "\n",
    "This returns the ethinicy of London. \n",
    "```python\n",
    "['White British (44.9%)',\n",
    " 'Other White (14.9%)',\n",
    " 'Asian (18.4%)',\n",
    " 'Black (13.3%)',\n",
    " 'Arab (1.3%)',\n",
    " 'Mixed (5%)',\n",
    " 'Other (2.2%)']\n",
    "```\n",
    "If the text was cleaner we could use `.extract` rather then `.re` to get the text we wanted. Once you've build up some expression that extracts the data in the shell you can copy it into your spider script, which we will crate next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the spiders folder we then need to define a spider to scrap the articles. A spider is a python class that subclasses one of the scrapy spiders, in this case the we'll use the `CrawlSpider`. Scrapy provides a command line tool to help generate the template for a spider. To generate spider template cd into the spider folder and run the following.\n",
    "\n",
    "```\n",
    "scrapy genspider -t crawl article en.wikipedia.org\n",
    "```\n",
    "\n",
    "For more info run:\n",
    "\n",
    "```\n",
    "scrapy genspider --help\n",
    "```\n",
    "\n",
    "Modify the spider code so it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from wikiSpider.items import Article\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class ArticleSpider(CrawlSpider): #\n",
    "    \n",
    "    name = \"article\" #spider name\n",
    "    allowed_domains = [\"en.wikipedia.org\"]#only follow links from this domain\n",
    "    start_urls = [\"http://en.wikipedia.org/wiki/Python_%28programming_language%29\"]\n",
    "    rules = [ \n",
    "        #Link extractor uses a regex to define which links to follow\n",
    "        Rule(LinkExtractor(allow=('(/wiki/)((?!:).)*$'),), callback=\"parse_item\", follow=True)\n",
    "    ]\n",
    "\n",
    "    #Spiders can take custom settings, these are high priority then the setting.py\n",
    "    custom_settings = {\n",
    "        'DEPTH_LIMIT': 1,\n",
    "        'DOWNLOAD_DELAY' : 0.25\n",
    "    }\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        item = Article() \n",
    "        title = response.css('#firstHeading::text').extract_first()\n",
    "        content =  response.css('#toc *::text').extract()\n",
    "        print(\"Title is: \"+title)\n",
    "        item['title'] = title\n",
    "        item['content'] = content\n",
    "        return item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the top part of the class defines the crawling logic essentially which page to vists, whereas the code in `parse_item` is the parsing logic, what items in that page are we intrested in.  Lets break down the top half of the code in more detial. \n",
    "\n",
    "* `name` - is a string which we can use to call our spider on the command line.\n",
    "* `custom_settings` - contains additional setting specific to this spider.\n",
    "* `allowed_domains` - is a list of domains which we'll let our spider vist. \n",
    "* `start_urls` - url of the page or pages to start on. \n",
    "* [LinkExtractor](https://doc.scrapy.org/en/latest/topics/link-extractors.html) - takes a regex or a list of regexes, that will match the links we wish to extract.\n",
    "* `Rule` - The rule object takes a LinkExtractor and two other arguments.  The `callback` argument takes a function which will be called on each page to excute our parsing logic. And the final argument `follow` which is a boolean which specifies if links should be followed from each response extracted, it defualts to True. \n",
    "\n",
    "In the second half we create a new instance of our `Article` class and add the h1 text to the title field. We are using `css` selectors to extract the data. Finally we return the item, we could use command line flags or a pipe line to write the item to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the spider by running the bellow in the terminal from the project directory.\n",
    "\n",
    "```\n",
    "scrapy crawl article\n",
    "```\n",
    "\n",
    "Note `article` is the name we put in are `ArticleSpider` class. You can stop the spider with `Ctrl+C` or `Ctrl+D` depending on your OS (operation system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrappy produces alot of debugging information, often a little to much. There are 5 levels to this info:\n",
    "\n",
    "* CRITICAL\n",
    "* ERROR\n",
    "* WARNING\n",
    "* DEBUG\n",
    "* INFO\n",
    "\n",
    "We can change the log level by putting `LOG_LEVEL = \"ERROR\"` in the settings.py Or we can pass it as a cmd flag.\n",
    "\n",
    "```\n",
    "scrapy crawl --logfile=\"wiki.log\" --loglevel=\"ERROR\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy uses the `Item` object to determine which information to save. You can use the bellow commands to output the item to csv or json.\n",
    "\n",
    "```\n",
    " scrapy crawl article -o articles.csv -t csv\n",
    " scrapy crawl article -o articles.json -t json\n",
    " \n",
    "```\n",
    "\n",
    "For Longer crawls we may want to pause it and resume at a later point, this can be acheieved by running:\n",
    "\n",
    "```\n",
    "scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n",
    "```\n",
    "\n",
    "We can stop the spider with Ctrl+C, and the restart it later by running the same command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Another option is to save the item using a pipeline.  Pipelines are often used to further process `Items` which we extracted with our spider, typicall usecases include:\n",
    "* Cleaning the scrapped data\n",
    "* Removing duplicate data.\n",
    "* Save database to a database or file. \n",
    "\n",
    "\n",
    "In order to activate a pipeline you need to add the bellow code to the settings.py file.\n",
    "\n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "    'wikiSpider.pipelines.CsvPipeline': 500,\n",
    "}\n",
    "```\n",
    "\n",
    "We'll use a pipeline to clean up the content menu data and write it to a csv with along with the page title.  The items 'keys' will be used as the column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "from scrapy.exporters import CsvItemExporter\n",
    "\n",
    "class CsvPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('output.csv', 'w+b')\n",
    "        self.exporter = CsvItemExporter(self.file)\n",
    "        self.exporter.start_exporting()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        #remove all spaces\n",
    "        item['content'] = [ s for s in item['content'] if not s.isspace() ]\n",
    "        self.exporter.export_item(item)\n",
    "        return item\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxies\n",
    "\n",
    "Proxies allow us to speed up scraping as we can make many concurrect request without the risk of getting our personal IP ban. If you want to use proxies with scrapy you can you the [scrapy-proxies](https://github.com/aivarsk/scrapy-proxies) middleware. Bellow is an example python script you can use to generate a list of proxies that can be used with scrapy-proxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "headers = {'User-Agent':\"Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0\"}\n",
    "proxy_url = \"https://free-proxy-list.net/\"\n",
    "r = requests.get(proxy_url, headers=headers)\n",
    "    \n",
    "path = os.getcwd()\n",
    "path = os.path.join('proxies.txt')\n",
    "\n",
    "if r.status_code == 200:\n",
    "\n",
    "    print('Request Successful')\n",
    "\n",
    "    dfs = pd.read_html(r.text)\n",
    "    df = dfs[0]\n",
    "    df = df[df.Https == \"yes\"]\n",
    "\n",
    "    proxies = []\n",
    "\n",
    "    for _,row in df[[\"IP Address\",\"Port\"]].iterrows():\n",
    "        ip = row[\"IP Address\"]\n",
    "        port = row[\"Port\"]\n",
    "        s = f\"https://{ip}:{int(port)}\"\n",
    "        proxies.append(s)\n",
    "\n",
    "    print(f'Writing {len(proxies)} proxies to {path}')\n",
    "\n",
    "    with open(path,\"w\") as f:\n",
    "        f.write(\"\\n\".join(proxies))\n",
    "else:\n",
    "    print(f'Request failed with status code{r.status_code}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "* Extract all of the quotes and author names from [Quotes to Scrape](http://quotes.toscrape.com/), and write to a csv file.\n",
    "* Pick a website of your choice and scrape it using scrapy. Make sure that the page doesn't dynamically load the data with javascript. This can be achieved by right clicking on the page and then `View Page Source`, if the data is there then you can scrapy without the hassle of dynamic javascript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* [Web Scrapping in python using scrapy](https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/)\n",
    "* [Getting Started with Web Scrapping using Scrapy](https://www.youtube.com/watch?v=vkA1cWN4DEc&list=PLZyvi_9gamL-EE3zQJbU5N3nzJcfNeFHU&t=0s&index=1)\n",
    "* [Scrapy Resources](https://scrapy.org/resources/)\n",
    "* [Scrapy exporting json and csv](http://www.scrapingauthority.com/2016/09/19/scrapy-exporting-json-and-csv/)\n",
    "* [Xpath and CSS selectors](https://devhints.io/xpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

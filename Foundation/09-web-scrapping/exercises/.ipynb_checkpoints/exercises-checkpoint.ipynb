{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "* ** 1. ** Play one of the CSS Selector Games.\n",
    "    * [CSS Diner](https://flukeout.github.io/)\n",
    "    * [CSS Leveler](http://toolness.github.io/css-selector-game/)\n",
    "* ** 2. ** Write a script that uses pandas `.read_html` to get all of the proxies from  https://free-proxy-list.net/ and write them to file.\n",
    "\n",
    "\n",
    "In the following exercises you may wish to use the proxies you gathered when you make request to the webpages, this can be done with:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "proxies = {\"http\": \"http://10.10.1.10:3128\",\n",
    "           \"https\": \"https://10.10.1.10:1080\"}\n",
    "\n",
    "requests.get(\"http://example.org\", proxies=proxies)\n",
    "```\n",
    "\n",
    "* ** 3. ** Extract all of the title names and links from [Hacker News](https://news.ycombinator.com/)\n",
    "* ** 4. ** Extract the `src` path for all of the images on [Reddit](https://www.reddit.com/)\n",
    "\n",
    "\n",
    "* ** 5. ** Extract all of the quotes and author names from [Quotes to Scrape](http://quotes.toscrape.com/), and write to a csv file.\n",
    "\n",
    "* ** 6. ** Pick a website of your choice and scrape it using scrapy. Make sure that the page doesn't dynamically load the data with javascript. This can be achieved by right clicking on the page and then `View Page Source`, if the data is there then you can scrapy without the hassle of dynamic javascript."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

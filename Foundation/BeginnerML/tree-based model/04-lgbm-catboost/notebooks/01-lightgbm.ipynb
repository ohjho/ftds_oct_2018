{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Light GBM?\n",
    "Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.\n",
    "\n",
    "Since it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.\n",
    "\n",
    "* XGBoost grow trees in the following manner\n",
    "\n",
    "![](img/xgboost_tree_growth.png)\n",
    "\n",
    "\n",
    "* LightGBM grow trees in the follwing manner\n",
    "\n",
    "![](img/lightgbm_tree_growth.png)\n",
    "\n",
    "\n",
    "Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max-depth which specifies the depth to which splitting will occur.\n",
    "\n",
    "Below, we will see the steps to install Light GBM and run a model using it. We will be comparing the results with XGBOOST results to prove that you should take Light GBM in a ‘LIGHT MANNER’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Parameters of light GBM\n",
    "\n",
    "* task : default value = train ; options = train , prediction ; Specifies the task we wish to perform which is either train or prediction.\n",
    "\n",
    "* application: default=regression, type=enum, options= options :\n",
    "\n",
    "            regression : perform regression task\n",
    "            binary : Binary classification\n",
    "            multiclass: Multiclass Classification\n",
    "            lambdarank : lambdarank application\n",
    "\n",
    "* data: type=string; training data , LightGBM will train from this data\n",
    "\n",
    "* num_iterations: number of boosting iterations to be performed ; default=100; type=int\n",
    "\n",
    "* num_leaves : number of leaves in one tree ; default = 31 ; type =int\n",
    "\n",
    "* device : default= cpu ; options = gpu,cpu. Device on which we want to train our model. Choose GPU for faster training.\n",
    "\n",
    "* max_depth: Specify the max depth to which tree will grow. This parameter is used to deal with overfitting.\n",
    "\n",
    "* min_data_in_leaf: Min number of data in one leaf.\n",
    "\n",
    "* feature_fraction: default=1 ; specifies the fraction of features to be taken for each iteration\n",
    "\n",
    "* bagging_fraction: default=1 ; specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting.\n",
    "\n",
    "* min_gain_to_split: default=.1 ; min gain to perform splitting\n",
    "\n",
    "* max_bin : max number of bins to bucket the feature values.\n",
    "\n",
    "* min_data_in_bin : min number of data in one bin\n",
    "\n",
    "* num_threads: default=OpenMP_default, type=int ;Number of threads for Light GBM.\n",
    "\n",
    "* label : type=string ; specify the label column\n",
    "\n",
    "* categorical_feature : type=string ; specify the categorical features we want to use for training our model\n",
    "\n",
    "* num_class: default=1 ; type=int ; used only for multi-class classification\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Parameters of Light GBM\n",
    "\n",
    "For best fit\n",
    "\n",
    "* num_leaves : This parameter is used to set the number of leaves to be formed in a tree. Theoretically relation between num_leaves and max_depth is num_leaves= 2^(max_depth). However, this is not a good estimate in case of Light GBM since splitting takes place leaf wise rather than depth wise. Hence num_leaves set must be smaller than 2^(max_depth) otherwise it may lead to overfitting. Light GBM does not have a direct relation between num_leaves and max_depth and hence the two must not be linked with each other.\n",
    "\n",
    "* min_data_in_leaf : It is also one of the important parameters in dealing with overfitting. Setting its value smaller may cause overfitting and hence must be set accordingly. Its value should be hundreds to thousands of large datasets.\n",
    "\n",
    "* max_depth: It specifies the maximum depth or level up to which tree can grow.\n",
    "\n",
    "\n",
    "For faster speed\n",
    "\n",
    "* bagging_fraction : Is used to perform bagging for faster results\n",
    "* feature_fraction : Set fraction of the features to be used at each iteration\n",
    "* max_bin : Smaller value of max_bin can save much time as it buckets the feature values in discrete bins which is computationally inexpensive.\n",
    "\n",
    "\n",
    "For better accuracy\n",
    "\n",
    "* Use bigger training data\n",
    "\n",
    "* num_leaves : Setting it to high value produces deeper trees with increased accuracy but lead to overfitting. Hence its higher value is not preferred.\n",
    "\n",
    "* max_bin : Setting it to high values has similar effect as caused by increasing value of num_leaves and also slower our training procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\brian tsang\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\brian tsang\\anaconda3\\lib\\site-packages (from lightgbm) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\brian tsang\\anaconda3\\lib\\site-packages (from lightgbm) (0.19.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\brian tsang\\anaconda3\\lib\\site-packages (from lightgbm) (1.14.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital_Status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_Status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  Income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing standard libraries \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame \n",
    "\n",
    "#import lightgbm \n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "#loading our training dataset 'adult.csv' with name 'data' using pandas \n",
    "data=pd.read_csv('data/adult.csv',header=None) \n",
    "\n",
    "#Assigning names to the columns \n",
    "data.columns=['age','workclass','fnlwgt','education','education-num','marital_Status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','Income'] \n",
    "\n",
    "#glimpse of the dataset \n",
    "data.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24720\n",
       "1     7841\n",
       "Name: Income, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoding our target variable \n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "l=LabelEncoder() \n",
    "l.fit(data.Income) \n",
    "\n",
    "l.classes_ \n",
    "data.Income=Series(l.transform(data.Income))  #label encoding our target variable \n",
    "data.Income.value_counts() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding of the Categorical features \n",
    "one_hot_workclass=pd.get_dummies(data.workclass) \n",
    "one_hot_education=pd.get_dummies(data.education) \n",
    "one_hot_marital_Status=pd.get_dummies(data.marital_Status) \n",
    "one_hot_occupation=pd.get_dummies(data.occupation)\n",
    "one_hot_relationship=pd.get_dummies(data.relationship) \n",
    "one_hot_race=pd.get_dummies(data.race) \n",
    "one_hot_sex=pd.get_dummies(data.sex) \n",
    "one_hot_native_country=pd.get_dummies(data.native_country) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing categorical features \n",
    "data.drop(['workclass','education','marital_Status','occupation','relationship','race','sex','native_country'],axis=1,inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['Income'], how='any', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging one hot encoded features with our dataset 'data' \n",
    "data=pd.concat([data,one_hot_workclass,one_hot_education,one_hot_marital_Status,one_hot_occupation,one_hot_relationship,one_hot_race,one_hot_sex,one_hot_native_country],axis=1) \n",
    "\n",
    "#Separating our data into features dataset x and our target dataset y \n",
    "x=data.drop('Income',axis=1) \n",
    "y=data.Income \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now splitting our dataset into test and train \n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=lgb.Dataset(x_train,label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters for lightgbm\n",
    "param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "param['metric'] = ['auc', 'binary_logloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training our model using light gbm\n",
    "num_round=50\n",
    "lgbm=lgb.train(param,train_data,num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02060282, 0.20703644, 0.04716318, 0.14248623, 0.64111057])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting on test set\n",
    "ypred2=lgbm.predict(x_test)\n",
    "ypred2[0:5]  # showing first 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting probabilities into 0 or 1\n",
    "for i in range(0,9769):\n",
    "    if ypred2[i]>=.5:       # setting threshold to .5\n",
    "       ypred2[i]=1\n",
    "    else:  \n",
    "       ypred2[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627844872727022"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating roc_auc_score for lightgbm\n",
    "auc_lgb =  roc_auc_score(y_test,ypred2)\n",
    "auc_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning solutions in competitions using lightGBM\n",
    "\n",
    "https://github.com/Microsoft/LightGBM/tree/master/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://lightgbm.readthedocs.io/en/latest/\n",
    "# https://github.com/Microsoft/LightGBM/tree/master/examples\n",
    "# https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
